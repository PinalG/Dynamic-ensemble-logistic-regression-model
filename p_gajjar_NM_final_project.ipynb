{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACgN_cm6wZ93"
   },
   "source": [
    "# Numerical Methods Final Project\n",
    "\n",
    "### Team Members\n",
    "1) Pinal Gajjar <br>\n",
    "2) Dharani Goalla <br>\n",
    "3) Venkata Narasa Reddy Boreddy <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnl3y9xXxCIK"
   },
   "source": [
    "## Task 1: Implement the simple logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlcthVkTnSvx",
    "outputId": "174d4835-06ce-4935-c371-2b58e4280a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood at iteration 0: 4522.761948452693\n",
      "Log Likelihood at iteration 100: 7062.677524559342\n",
      "Log Likelihood at iteration 200: 7044.520241800876\n",
      "Log Likelihood at iteration 300: 7044.658450518149\n",
      "Log Likelihood at iteration 400: 7044.657756027635\n",
      "Log Likelihood at iteration 500: 7044.657758614851\n",
      "Log Likelihood at iteration 600: 7044.6577586232215\n",
      "Log Likelihood at iteration 700: 7044.657758623096\n",
      "Log Likelihood at iteration 800: 7044.657758623096\n",
      "Log Likelihood at iteration 900: 7044.657758623096\n",
      "Log Likelihood at iteration 1000: 7044.657758623096\n",
      "Log Likelihood at iteration 1100: 7044.657758623096\n",
      "Log Likelihood at iteration 1200: 7044.657758623096\n",
      "Log Likelihood at iteration 1300: 7044.657758623096\n",
      "Log Likelihood at iteration 1400: 7044.657758623096\n",
      "Log Likelihood at iteration 1500: 7044.657758623096\n",
      "Log Likelihood at iteration 1600: 7044.657758623096\n",
      "Log Likelihood at iteration 1700: 7044.657758623096\n",
      "Log Likelihood at iteration 1800: 7044.657758623096\n",
      "Log Likelihood at iteration 1900: 7044.657758623096\n",
      "Log Likelihood at iteration 2000: 7044.657758623096\n",
      "Log Likelihood at iteration 2100: 7044.657758623096\n",
      "Log Likelihood at iteration 2200: 7044.657758623096\n",
      "Log Likelihood at iteration 2300: 7044.657758623096\n",
      "Log Likelihood at iteration 2400: 7044.657758623096\n",
      "Log Likelihood at iteration 2500: 7044.657758623096\n",
      "Log Likelihood at iteration 2600: 7044.657758623096\n",
      "Log Likelihood at iteration 2700: 7044.657758623096\n",
      "Log Likelihood at iteration 2800: 7044.657758623096\n",
      "Log Likelihood at iteration 2900: 7044.657758623096\n",
      "Log Likelihood at iteration 3000: 7044.657758623096\n",
      "Log Likelihood at iteration 3100: 7044.657758623096\n",
      "Log Likelihood at iteration 3200: 7044.657758623096\n",
      "Log Likelihood at iteration 3300: 7044.657758623096\n",
      "Log Likelihood at iteration 3400: 7044.657758623096\n",
      "Log Likelihood at iteration 3500: 7044.657758623096\n",
      "Log Likelihood at iteration 3600: 7044.657758623096\n",
      "Log Likelihood at iteration 3700: 7044.657758623096\n",
      "Log Likelihood at iteration 3800: 7044.657758623096\n",
      "Log Likelihood at iteration 3900: 7044.657758623096\n",
      "Log Likelihood at iteration 4000: 7044.657758623096\n",
      "Log Likelihood at iteration 4100: 7044.657758623096\n",
      "Log Likelihood at iteration 4200: 7044.657758623096\n",
      "Log Likelihood at iteration 4300: 7044.657758623096\n",
      "Log Likelihood at iteration 4400: 7044.657758623096\n",
      "Log Likelihood at iteration 4500: 7044.657758623096\n",
      "Log Likelihood at iteration 4600: 7044.657758623096\n",
      "Log Likelihood at iteration 4700: 7044.657758623096\n",
      "Log Likelihood at iteration 4800: 7044.657758623096\n",
      "Log Likelihood at iteration 4900: 7044.657758623096\n",
      "Accuracy of the simple logistic regression model: 0.65\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sigmoid function for logistic regression\n",
    "def sigmoid(z):\n",
    "    # Clip z to prevent overflow.\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Logistic regression probability function\n",
    "def logistic_regression_prob(X, theta):\n",
    "    return sigmoid(np.dot(X, theta))\n",
    "\n",
    "# Log likelihood for a simple logistic regression\n",
    "def log_likelihood_simple(theta, X, y):\n",
    "    predictions = logistic_regression_prob(X, theta)\n",
    "    epsilon = 1e-5  # Avoid log(0)\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    log_likelihood_value = np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "    return -log_likelihood_value  # Minimize negative log likelihood\n",
    "\n",
    "# Gradient for a simple logistic regression\n",
    "def log_likelihood_gradient_simple(theta, X, y):\n",
    "    predictions = logistic_regression_prob(X, theta)\n",
    "    errors = y - predictions\n",
    "    gradient = np.dot(X.T, errors)\n",
    "    return gradient\n",
    "\n",
    "# Gradient ascent optimization for simple logistic regression\n",
    "def gradient_ascent_simple(X, y, learning_rate, iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for i in range(iterations):\n",
    "        gradient = log_likelihood_gradient_simple(theta, X, y)\n",
    "        theta += learning_rate * gradient\n",
    "        if i % 100 == 0:  # Print log likelihood every 100 iterations\n",
    "            print(f'Log Likelihood at iteration {i}: {log_likelihood_simple(theta, X, y)}')\n",
    "    return theta\n",
    "\n",
    "# Load and prepare your data\n",
    "wine_data_path = 'https://raw.githubusercontent.com/PinalG/Dynamic-ensemble-logistic-regression-model/main/allwine.csv'  # Replace with the correct path\n",
    "wine_data = pd.read_csv(wine_data_path)\n",
    "\n",
    "# Prepare the data\n",
    "X = wine_data.drop('quality', axis=1)\n",
    "y = wine_data['quality']\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])  # Add intercept term\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TASK 1: Simple Logistic Regression\n",
    "learning_rate = 0.01\n",
    "iterations = 5000\n",
    "theta_simple = gradient_ascent_simple(X_train, y_train, learning_rate, iterations)\n",
    "\n",
    "# Make predictions using the simple logistic regression model\n",
    "predictions_simple = logistic_regression_prob(X_test, theta_simple) >= 0.5\n",
    "\n",
    "# Calculate accuracy for the simple logistic regression model\n",
    "accuracy_simple = accuracy_score(y_test, predictions_simple)\n",
    "print(f'Accuracy of the simple logistic regression model: {accuracy_simple}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoOSXLfEyJ10"
   },
   "source": [
    "## Task 2: Dynamic Ensemble Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZY9LJ8vxulM",
    "outputId": "ca877135-16a1-42ac-e74a-6d71873be17c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the ensemble model: 0.8125\n"
     ]
    }
   ],
   "source": [
    "def log_likelihood_gradient(theta, X, y):\n",
    "    theta_M = theta[:X.shape[1]]\n",
    "    theta_L = theta[X.shape[1]:2*X.shape[1]]\n",
    "    theta_R = theta[2*X.shape[1]:]\n",
    "\n",
    "    h_thetaM = logistic_regression_prob(X, theta_M)\n",
    "    h_thetaL = logistic_regression_prob(X, theta_L)\n",
    "    h_thetaR = logistic_regression_prob(X, theta_R)\n",
    "\n",
    "    common_term = y - (h_thetaL * h_thetaM + h_thetaR * (1 - h_thetaM))\n",
    "    grad_M = np.dot(X.T, common_term * (h_thetaL - h_thetaR))\n",
    "    grad_L = np.dot(X.T, common_term * h_thetaM)\n",
    "    grad_R = np.dot(X.T, -common_term * (1 - h_thetaM))\n",
    "\n",
    "    # Combine gradients\n",
    "    grad = np.concatenate([grad_M, grad_L, grad_R])\n",
    "\n",
    "    return grad\n",
    "\n",
    "# Probability function for the ensemble model\n",
    "def probability_function(X, theta_M, theta_L, theta_R, y):\n",
    "    h_thetaM = logistic_regression_prob(X, theta_M)\n",
    "    h_thetaL = logistic_regression_prob(X, theta_L)\n",
    "    h_thetaR = logistic_regression_prob(X, theta_R)\n",
    "\n",
    "    P_1 = h_thetaL * h_thetaM + h_thetaR * (1 - h_thetaM)\n",
    "    P_0 = (1 - h_thetaL) * h_thetaM + (1 - h_thetaR) * (1 - h_thetaM)\n",
    "\n",
    "    return np.where(y == 1, P_1, P_0)\n",
    "\n",
    "# Gradient ascent for logistic regression\n",
    "def logistic_regression_gradient_ascent(X, y, theta, learning_rate, iterations):\n",
    "    for i in range(iterations):\n",
    "        grad = logistic_regression_gradient(theta, X, y)\n",
    "        theta += learning_rate * grad\n",
    "    return theta\n",
    "\n",
    "# Gradient ascent for the ensemble model\n",
    "def gradient_ascent_ensemble(X, y, theta, learning_rate, iterations):\n",
    "    for i in range(iterations):\n",
    "        grad = log_likelihood_gradient(theta, X, y)\n",
    "        theta += learning_rate * grad\n",
    "        # if i % 100 == 0:  # Print log likelihood every 100 iterations\n",
    "        #     print(f'Log Likelihood at iteration {i}: {log_likelihood_ensemble(theta, X, y)}')\n",
    "    return theta\n",
    "\n",
    "\n",
    "theta_initial = np.zeros(X_train.shape[1] * 3)\n",
    "\n",
    "# Hyperparameters for gradient ascent\n",
    "learning_rate = 0.0028  # tune learning rates\n",
    "iterations = 10000  # tune number of iterations\n",
    "\n",
    "# Run gradient ascent to maximize the log likelihood for the ensemble model\n",
    "theta_optimal_ensemble = gradient_ascent_ensemble(X_train, y_train, theta_initial, learning_rate, iterations)\n",
    "\n",
    "# Extract the optimized parameters for each model\n",
    "theta_M_ensemble = theta_optimal_ensemble[:X_train.shape[1]]\n",
    "theta_L_ensemble = theta_optimal_ensemble[X_train.shape[1]:2*X_train.shape[1]]\n",
    "theta_R_ensemble = theta_optimal_ensemble[2*X_train.shape[1]:]\n",
    "\n",
    "# Make predictions using the ensemble model\n",
    "ensemble_probs_task2 = probability_function(X_test, theta_M_ensemble, theta_L_ensemble, theta_R_ensemble, y_test)\n",
    "ensemble_preds_task2 = ensemble_probs_task2 >= 0.5\n",
    "\n",
    "# Calculate accuracy for the ensemble model\n",
    "accuracy_ensemble_task2 = accuracy_score(y_test, ensemble_preds_task2)\n",
    "print(f'Accuracy of the ensemble model: {accuracy_ensemble_task2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFclMWtyydMd"
   },
   "source": [
    "## Task 3: Deep Dynamic Ensemble Model Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niGUUkeFybUC",
    "outputId": "8c3740db-00a7-4e01-b308-438c253874b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Log Likelihood: 0.6875286323505992\n",
      "Iteration 1000, Log Likelihood: 0.5343642298977115\n",
      "Iteration 2000, Log Likelihood: 0.5339710835519595\n",
      "Iteration 3000, Log Likelihood: 0.5339349564410933\n",
      "Iteration 4000, Log Likelihood: 0.5339483107905347\n",
      "Iteration 5000, Log Likelihood: 0.5339653465958092\n",
      "Iteration 6000, Log Likelihood: 0.5339774407885458\n",
      "Iteration 7000, Log Likelihood: 0.533984793507454\n",
      "Iteration 8000, Log Likelihood: 0.5339889871570344\n",
      "Iteration 9000, Log Likelihood: 0.533991307773959\n",
      "Accuracy of the deep ensemble model: 0.7484375\n"
     ]
    }
   ],
   "source": [
    "# Gradient of log likelihood for the three-layer ensemble model\n",
    "def gradient_ensemble(thetas, X, y, lambda_reg=0.01):\n",
    "    theta_1, theta_2, theta_3, theta_4, theta_5, theta_6, theta_7 = np.split(thetas, 7)\n",
    "    p_1 = logistic_regression_prob(X, theta_1)\n",
    "    p_2 = logistic_regression_prob(X, theta_2)\n",
    "    p_3 = logistic_regression_prob(X, theta_3)\n",
    "    p_4 = logistic_regression_prob(X, theta_4)\n",
    "    p_5 = logistic_regression_prob(X, theta_5)\n",
    "    p_6 = logistic_regression_prob(X, theta_6)\n",
    "    p_7 = logistic_regression_prob(X, theta_7)\n",
    "\n",
    "    final_prediction = (p_4 * p_2 * p_1 +\n",
    "                        p_5 * (1 - p_2) * p_1 +\n",
    "                        p_6 * p_3 * (1 - p_1) +\n",
    "                        p_7 * (1 - p_3) * (1 - p_1))\n",
    "    final_prediction = np.clip(final_prediction, 1e-10, 1 - 1e-10)\n",
    "\n",
    "    error = y - final_prediction\n",
    "    grad_theta_1 = np.dot(X.T, error * ((p_2 * p_4 + (1 - p_2) * p_5) - (p_3 * p_6 + (1 - p_3) * p_7))) + lambda_reg * theta_1\n",
    "    grad_theta_2 = np.dot(X.T, error * p_1 * (p_4 - p_5)) + lambda_reg * theta_2\n",
    "    grad_theta_3 = np.dot(X.T, error * (1 - p_1) * (p_6 - p_7)) + lambda_reg * theta_3\n",
    "    grad_theta_4 = np.dot(X.T, error * p_2 * p_1) + lambda_reg * theta_4\n",
    "    grad_theta_5 = np.dot(X.T, error * (1 - p_2) * p_1) + lambda_reg * theta_5\n",
    "    grad_theta_6 = np.dot(X.T, error * p_3 * (1 - p_1)) + lambda_reg * theta_6\n",
    "    grad_theta_7 = np.dot(X.T, error * (1 - p_3) * (1 - p_1)) + lambda_reg * theta_7\n",
    "\n",
    "    gradients = np.concatenate([grad_theta_1, grad_theta_2, grad_theta_3,\n",
    "                                grad_theta_4, grad_theta_5, grad_theta_6, grad_theta_7])\n",
    "    return gradients\n",
    "\n",
    "# Gradient ascent optimization function\n",
    "def gradient_ascent(X, y, learning_rate=0.00005, iterations=20000, gradient_function=gradient_ensemble, lambda_reg=1):\n",
    "    thetas = np.zeros(X.shape[1] * 7)  # Initialize parameters for all nodes\n",
    "    for i in range(iterations):\n",
    "        gradient = gradient_function(thetas, X, y, lambda_reg)\n",
    "        thetas += learning_rate * gradient\n",
    "\n",
    "        if i % 1000 == 0:  # Print log likelihood every 1000 iterations\n",
    "            # Compute the final prediction for log likelihood\n",
    "            theta_1, theta_2, theta_3, theta_4, theta_5, theta_6, theta_7 = np.split(thetas, 7)\n",
    "            p_1 = logistic_regression_prob(X, theta_1)\n",
    "            p_2 = logistic_regression_prob(X, theta_2)\n",
    "            p_3 = logistic_regression_prob(X, theta_3)\n",
    "            p_4 = logistic_regression_prob(X, theta_4)\n",
    "            p_5 = logistic_regression_prob(X, theta_5)\n",
    "            p_6 = logistic_regression_prob(X, theta_6)\n",
    "            p_7 = logistic_regression_prob(X, theta_7)\n",
    "\n",
    "            final_prediction = (p_4 * p_2 * p_1 +\n",
    "                                p_5 * (1 - p_2) * p_1 +\n",
    "                                p_6 * p_3 * (1 - p_1) +\n",
    "                                p_7 * (1 - p_3) * (1 - p_1))\n",
    "            final_prediction = np.clip(final_prediction, 1e-10, 1 - 1e-10)\n",
    "\n",
    "            ll = -np.mean(y * np.log(final_prediction) + (1 - y) * np.log(1 - final_prediction))\n",
    "            print(f'Iteration {i}, Log Likelihood: {ll}')\n",
    "\n",
    "    return thetas\n",
    "\n",
    "learning_rate_task3 = 0.0001  #   tune this\n",
    "iterations_task3 = 10000     #  tune this\n",
    "\n",
    "# Optimize the deep ensemble model\n",
    "thetas_optimized = gradient_ascent(\n",
    "    X_train, y_train,\n",
    "    learning_rate_task3, iterations_task3,\n",
    "    gradient_ensemble\n",
    ")\n",
    "\n",
    "# Extract the optimized parameters for each node\n",
    "theta_1, theta_2, theta_3, theta_4, theta_5, theta_6, theta_7 = np.split(thetas_optimized, 7)\n",
    "def compute_deep_ensemble_probs(X, theta_1, theta_2, theta_3, theta_4, theta_5, theta_6, theta_7):\n",
    "    p_1 = logistic_regression_prob(X, theta_1)\n",
    "    p_2 = logistic_regression_prob(X, theta_2)\n",
    "    p_3 = logistic_regression_prob(X, theta_3)\n",
    "    p_4 = logistic_regression_prob(X, theta_4)\n",
    "    p_5 = logistic_regression_prob(X, theta_5)\n",
    "    p_6 = logistic_regression_prob(X, theta_6)\n",
    "    p_7 = logistic_regression_prob(X, theta_7)\n",
    "\n",
    "    final_prediction = (p_4 * p_2 * p_1 +\n",
    "                        p_5 * (1 - p_2) * p_1 +\n",
    "                        p_6 * p_3 * (1 - p_1) +\n",
    "                        p_7 * (1 - p_3) * (1 - p_1))\n",
    "    final_prediction = np.clip(final_prediction, 1e-10, 1 - 1e-10)\n",
    "    return final_prediction\n",
    "\n",
    "# Make predictions using the deep ensemble model\n",
    "deep_ensemble_probs_task3 = compute_deep_ensemble_probs(X_test, theta_1, theta_2, theta_3, theta_4, theta_5, theta_6, theta_7)\n",
    "deep_ensemble_preds_task3 = deep_ensemble_probs_task3 >= 0.5\n",
    "\n",
    "# Calculate accuracy for the deep ensemble model\n",
    "accuracy_deep_ensemble_task3 = accuracy_score(y_test, deep_ensemble_preds_task3)\n",
    "print(f'Accuracy of the deep ensemble model: {accuracy_deep_ensemble_task3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6KZ6o9IypuX"
   },
   "source": [
    "## Task 4 Generalizing the model to support arbitrary number of layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozw6Q-yWGFbK",
    "outputId": "7df423dc-2a1c-4d83-813f-1bee79a915cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the generalized ensemble model: 0.7421875\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Probability Function\n",
    "def logistic_regression_prob(X, theta):\n",
    "    return 1 / (1 + np.exp(-np.dot(X, theta)))\n",
    "\n",
    "# Generalized Gradient of Log Likelihood for Ensemble Model\n",
    "def gradient_ensemble(thetas, X, y, num_layers, lambda_reg=0.01):\n",
    "    thetas_split = np.split(thetas, num_layers)\n",
    "    probs = [logistic_regression_prob(X, theta) for theta in thetas_split]\n",
    "\n",
    "    final_prediction = probs[0]\n",
    "    for i in range(1, num_layers):\n",
    "        final_prediction = final_prediction * probs[i] + (1 - final_prediction) * probs[i]\n",
    "    final_prediction = np.clip(final_prediction, 1e-10, 1 - 1e-10)\n",
    "\n",
    "    gradients = []\n",
    "    for i in range(num_layers):\n",
    "        error = y - final_prediction\n",
    "        partial_grad = np.dot(X.T, error * final_prediction * (1 - final_prediction)) + lambda_reg * thetas_split[i]\n",
    "        gradients.append(partial_grad)\n",
    "\n",
    "    return np.concatenate(gradients)\n",
    "\n",
    "# Gradient Ascent Optimization Function\n",
    "def gradient_ascent(X, y, num_layers, learning_rate=0.00005, iterations=20000, lambda_reg=1):\n",
    "    thetas = np.zeros(X.shape[1] * num_layers)\n",
    "    for i in range(iterations):\n",
    "        gradient = gradient_ensemble(thetas, X, y, num_layers, lambda_reg)\n",
    "        thetas += learning_rate * gradient\n",
    "        # Optional: Add log likelihood computation here\n",
    "    return thetas\n",
    "\n",
    "# Generalized Ensemble Prediction Function\n",
    "def predict_ensemble(X, thetas, num_layers):\n",
    "    thetas_split = np.split(thetas, num_layers)\n",
    "    final_prediction = logistic_regression_prob(X, thetas_split[0])\n",
    "    for i in range(1, num_layers):\n",
    "        final_prediction = final_prediction * logistic_regression_prob(X, thetas_split[i]) + (1 - final_prediction) * logistic_regression_prob(X, thetas_split[i])\n",
    "    return final_prediction\n",
    "\n",
    "# Load dataset\n",
    "wine_data = pd.read_csv('https://raw.githubusercontent.com/PinalG/Dynamic-ensemble-logistic-regression-model/main/allwine.csv')\n",
    "\n",
    "# Prepare data\n",
    "X = wine_data.drop('quality', axis=1)\n",
    "y = wine_data['quality']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set hyperparameters\n",
    "num_layers = 7  # Experiment with this value\n",
    "learning_rate = 0.001  # Adjust learning rate\n",
    "iterations = 15000  # Increase iterations\n",
    "lambda_reg = 0.01  # Experiment with regularization strength\n",
    "\n",
    "# Train model\n",
    "thetas_optimized = gradient_ascent(X_train, y_train, num_layers, learning_rate, iterations, lambda_reg)\n",
    "\n",
    "# Predict and evaluate\n",
    "ensemble_predictions = predict_ensemble(X_test, thetas_optimized, num_layers)\n",
    "predictions = np.where(ensemble_predictions >= 0.5, 1, 0)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of the generalized ensemble model: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Wzx3B6I_GJrC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
